{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHW4_Seq2Seq Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group_member = \"You Zhang & Changxu Zhang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 16\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "MIN_COUNT = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_lines.tsv', encoding='utf-8', errors='ignore') as f:\n",
    "    data = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "# count = 0\n",
    "for line in data[:-1]:\n",
    "    splitlist = line.rstrip().split(\"\\t\")\n",
    "#     count += 1\n",
    "    if splitlist[0].startswith('\"'):\n",
    "        splitlist[0] = splitlist[0][1:]\n",
    "        if splitlist[-1].endswith('\"'):\n",
    "            splitlist[-1] = splitlist[-1][:-1]\n",
    "    assert len(splitlist) > 3\n",
    "    if len(splitlist) == 4: \n",
    "        splitlist.append(\" \")\n",
    "    if len(splitlist) > 5:\n",
    "        splitlist = splitlist[:4] + [\" \".join(splitlist[4:])]\n",
    "    lines.append(splitlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = pd.DataFrame()\n",
    "movie_lines[\"my\"] = pd.Series(lines)\n",
    "movie_lines[['lineID','characterID', 'movieID', 'character name', 'text of the utterance']] \\\n",
    "   = pd.DataFrame(movie_lines.my.values.tolist(), index = movie_lines.index)\n",
    "movie_lines.drop(columns=\"my\", inplace = True)\n",
    "assert all(movie_lines[\"lineID\"].str.startswith('L'))\n",
    "assert all(movie_lines[\"characterID\"].str.startswith('u'))\n",
    "assert all(movie_lines[\"movieID\"].str.startswith('m'))\n",
    "movie_lines[\"lineID\"] = movie_lines[\"lineID\"].apply(lambda x: int(x[1:]))\n",
    "movie_lines[\"characterID\"] = movie_lines[\"characterID\"].apply(lambda x: int(x[1:]))\n",
    "movie_lines[\"movieID\"] = movie_lines[\"movieID\"].apply(lambda x: int(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\{.*?\\) \", \" \", text)\n",
    "    text = re.sub(r\"\\{.*?\\} \", \" \", text)\n",
    "    text = re.sub(r\"</.*?>\", \" \", text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"<.*?<\", \" \", text)\n",
    "    text = re.sub(r\"&.*?;\", \" \", text)\n",
    "    text = re.sub(r'[^\\d]%', \" \", text)\n",
    "    \n",
    "    try:\n",
    "        while re.match(r\"^[^a-zA-Z]+.*\", text).group() == text:\n",
    "            text = text[1:]\n",
    "    except:\n",
    "        pass\n",
    "    text = (text.lower()\n",
    "            .replace('*', '')\n",
    "            .replace('`', '')\n",
    "            .replace('+', '')\n",
    "            .replace('|', '')\n",
    "            .replace(']', ' ')\n",
    "            .replace('[', '')\n",
    "            .replace('<', '')\n",
    "            .replace('>', '.')\n",
    "            .replace('=', ' ')\n",
    "            .replace('~', '')\n",
    "            .replace('\\^', '')\n",
    "            .replace('--', ' ')\n",
    "            .replace('    ', ' ')\n",
    "            .replace('   ', ' ')\n",
    "            .replace('.', ' .')\n",
    "            .replace(':', ' :')\n",
    "            .replace(';', ' ;')\n",
    "            .replace('!', ' !')\n",
    "            .replace('?', ' ?')\n",
    "            .replace('  ', ' '))\n",
    "    return text\n",
    "\n",
    "movie_lines['text of the utterance'] = movie_lines['text of the utterance'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines.sort_values(by=['movieID', 'lineID'], inplace = True)\n",
    "movie_lines = movie_lines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Q&A Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Q&As: 154460\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "movieQandA = movie_lines['text of the utterance'].str.split(\" \").values\n",
    "characterID = movie_lines['characterID'].values\n",
    "movieID = movie_lines['movieID'].values\n",
    "\n",
    "for i in range(len(movieQandA) - 1):\n",
    "    if len(movieQandA[i]) < MAX_LENGTH \\\n",
    "        and len(movieQandA[i+1]) < MAX_LENGTH \\\n",
    "        and not characterID[i] == characterID[i+1] \\\n",
    "        and movieID[i] == movieID[i+1]:\n",
    "        questions.append(movieQandA[i])\n",
    "        answers.append(movieQandA[i+1])\n",
    "        \n",
    "print(\"number of Q&As:\", len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in questions: 32120\n",
      "number of all words: 36555\n"
     ]
    }
   ],
   "source": [
    "word2count = {}\n",
    "\n",
    "for question in questions:\n",
    "    for word in question:\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            \n",
    "print(\"number of words in questions:\", len(word2count))\n",
    "            \n",
    "for answer in answers:\n",
    "#     if not (answer in questions):\n",
    "    for word in answer:\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "print(\"number of all words:\", len(word2count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim and Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of keep words: 14503\n"
     ]
    }
   ],
   "source": [
    "keep_words = []\n",
    "for key, value in word2count.items():\n",
    "    if value >= MIN_COUNT:\n",
    "        keep_words.append(key)\n",
    "\n",
    "print(\"number of keep words:\", len(keep_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of keep Q&As: 124499\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "index2word = {0:\" \", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "num_of_words = len(index2word)\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(questions)):\n",
    "    keep1 = True\n",
    "    keep2 = True\n",
    "    for word1 in questions[i]:\n",
    "        if not word1 in keep_words:\n",
    "            keep1 = False\n",
    "            break\n",
    "    for word2 in answers[i]:\n",
    "        if not word2 in keep_words:\n",
    "            keep2 = False\n",
    "            break\n",
    "    if keep1 and keep2:\n",
    "        pairs.append([\" \".join(questions[i]), \" \".join(answers[i])])\n",
    "\n",
    "print(\"number of keep Q&As:\", len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pairs) - 1):\n",
    "    for word in pairs[i][0].split(\" \"):\n",
    "        if word not in word2index:\n",
    "            word2index[word] = num_of_words\n",
    "            index2word[num_of_words] = word\n",
    "            num_of_words += 1\n",
    "    if not pairs[i][1] == pairs[i+1][0]:\n",
    "        for word in pairs[i][1].split(\" \"):\n",
    "            if word not in word2index:\n",
    "                word2index[word] = num_of_words\n",
    "                index2word[num_of_words] = word\n",
    "                num_of_words += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Model\n",
    "Referrence: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(word2index, sentence):\n",
    "    return [word2index[word] for word in sentence.split(\" \")] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=0):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == 0:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, word2index):\n",
    "    indexes_batch = [indexesFromSentence(word2index, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, word2index):\n",
    "    indexes_batch = [indexesFromSentence(word2index, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(word2index, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, word2index)\n",
    "    output, mask, max_target_len = outputVar(output_batch, word2index)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat(nn.Module):\n",
    "    def __init__(self,embedding,hidden_size,num_of_words,dropout=0.1,batch_size = 64):\n",
    "        super(Chat,self).__init__()\n",
    "        self.encode_layers = 2\n",
    "        self.decode_layers = 2\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = num_of_words\n",
    "        self.lstm_ecd = nn.GRU(input_size = hidden_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = self.encode_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional=True\n",
    "                           )\n",
    "        self.lstm_dcd = nn.GRU(input_size = hidden_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = self.decode_layers,\n",
    "                            dropout = dropout\n",
    "                           )\n",
    "        self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,num_of_words)\n",
    "        self.batch_size = batch_size\n",
    "        self.teaching = 1.0\n",
    "        \n",
    "    def attention(self,hidden,ecd_out):\n",
    "        energy = torch.sum(hidden*ecd_out,dim=2).t()\n",
    "        return F.softmax(energy,dim=1).unsqueeze(1)\n",
    "    def encode(self,x,length,h = None):\n",
    "        embed = self.embedding(x)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embed,length)\n",
    "        out,h = self.lstm_ecd(packed,h)\n",
    "        out,_ = torch.nn.utils.rnn.pad_packed_sequence(out)\n",
    "        out = out[:,:,:self.hidden_size]+out[:,:,self.hidden_size:]\n",
    "        return out,h\n",
    "    def decode(self,ipt,last_hidden,encoder_out):\n",
    "        embed = self.embedding(ipt)\n",
    "        out,hidden = self.lstm_dcd(embed,last_hidden)\n",
    "        attn_weights = self.attention(out,encoder_out)\n",
    "        context = attn_weights.bmm(encoder_out.transpose(0,1))\n",
    "        out = out.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_ipt = torch.cat((out,context),1)\n",
    "        concat_out = torch.tanh(self.concat(concat_ipt))\n",
    "        output = self.out(concat_out)\n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output,hidden\n",
    "    def eval_result(self,ipt_seq,length,max_length):\n",
    "        encoder_out,encoder_hidden = self.encode(ipt_seq,length)\n",
    "        decoder_hidden = encoder_hidden[:self.decode_layers]\n",
    "        decoder_input = torch.ones(1,1,device=device,dtype=torch.long)*SOS_token\n",
    "        all_tokens = torch.zeros([0],device=device,dtype=torch.long)\n",
    "        all_scores = torch.zeros([0],device=device)\n",
    "        for i in range(max_length):\n",
    "            decoder_output,decoder_hidden = self.decode(decoder_input,decoder_hidden,encoder_out)\n",
    "            decoder_scores,decoder_input = torch.max(decoder_output,dim=1)\n",
    "            all_tokens = torch.cat((all_tokens,decoder_input),dim=0)\n",
    "            all_scores = torch.cat((all_scores,decoder_scores),dim=0)\n",
    "            decoder_input = torch.unsqueeze(decoder_input,0)\n",
    "        return all_tokens,all_scores\n",
    "    \n",
    "    def forward(self,input_variable,length,target,mask,max_target_length):\n",
    "        encoder_outputs,encoder_hidden = self.encode(input_variable,length)\n",
    "        decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        decoder_hidden = encoder_hidden[:self.decode_layers]\n",
    "        return_list = []\n",
    "        for t in range(max_target_length):\n",
    "            decoder_out,decoder_hidden = self.decode(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            decoder_input=target[t].view(1,-1)\n",
    "            return_list.append(decoder_out)\n",
    "        return return_list\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def train(pairs,num_of_words):\n",
    "    batch_size= 64\n",
    "    n_iteration = 4\n",
    "    hidden_size = 500\n",
    "    clip=50\n",
    "    training_batches = [batch2TrainData(word2index, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    embedding = nn.Embedding(num_of_words, hidden_size)\n",
    "    \n",
    "    chat_model = Chat(embedding,hidden_size,num_of_words).to(device)\n",
    "    chat_model.train()\n",
    "    optimizer = optim.Adam(chat_model.parameters(),lr=0.0001)\n",
    "    print(\"Training...\")\n",
    "    loss_all = 0\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        input_variable = input_variable.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        target_variable = target_variable.to(device)\n",
    "        mask = mask.to(device)\n",
    "        result_list = chat_model(input_variable, lengths, target_variable,mask,max_target_len)\n",
    "        loss = 0\n",
    "        nTotal= 0\n",
    "        for t in range(max_target_len):\n",
    "            mask_loss,nTotal = maskNLLLoss(result_list[t],target_variable[t],mask[t])\n",
    "            loss+=mask_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        _ = torch.nn.utils.clip_grad_norm_(chat_model.parameters(),clip)\n",
    "        optimizer.step()\n",
    "        print(\"Iteration: %d\"%iteration,\"LOSS:\",loss/nTotal)\n",
    "    return chat_model\n",
    "        \n",
    "# Referrence: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
    "def evaluate(chat_model,index2word,word2index, sentence, max_length=MAX_LENGTH):\n",
    "    indexes_batch = [indexesFromSentence(word2index, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    tokens, scores = chat_model.eval_result(input_batch, lengths, max_length)\n",
    "    decoded_words = [index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(chat_model, index2word,word2index):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            input_sentence = input('Human > ')\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            input_sentence = clean_text(input_sentence)\n",
    "            output_words = evaluate(chat_model,index2word,word2index, input_sentence)\n",
    "            output_words[:] = [x for x in output_words if not x == 'EOS']\n",
    "            print('Bot >', ' '.join(output_words))\n",
    "        except:\n",
    "            print('Sorry, I don\\'t know what you mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "chat_model = train(pairs,num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.eval()\n",
    "evaluateInput(chat_model,index2word,word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "#### https://github.com/Conchylicultor/DeepQA/\n",
    "#### https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
    "#### https://github.com/Currie32/Chatbot-from-Movie-Dialogue/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
